{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from ast import literal_eval\n",
    "\n",
    "'''\n",
    "    NOTE: When this program is executed for the first time it takes around a 1 minute to complete because of \n",
    "    time taken involved to import libraries.\n",
    "    When it is executed again, it takes 25-30secs to complete.\n",
    "'''\n",
    "\n",
    "# Class Definition\n",
    "class AnomalyDetection():\n",
    "\n",
    "    def scaleNum(self, df, indices):\n",
    "\n",
    "        # Getting the subset of columns to be scaled\n",
    "        non_scaled_cols = df['features'].apply(lambda row: [row[i] for i in indices])\n",
    "        non_scaled_split_df = pd.DataFrame(non_scaled_cols.tolist(), index=df.index, columns=indices)\n",
    "\n",
    "        # Standardization\n",
    "        non_scaled_split_df = (non_scaled_split_df - non_scaled_split_df.mean()) / non_scaled_split_df.std()\n",
    "\n",
    "        # copy scaled values to original dataframe\n",
    "        def copyScaledVaues(row):\n",
    "            for index in indices:\n",
    "                row['features'][index] = non_scaled_split_df.loc[row.name, index]\n",
    "\n",
    "        df.apply(copyScaledVaues, axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cat2Num(self, df, indices):\n",
    "\n",
    "        # Seperating categorical columns and other columns\n",
    "        catdf = df['features'].apply(lambda row: [row[i] for i in indices])\n",
    "        noncat_df = df['features'].apply(lambda row: [row[i] for i in range(len(row)) if i not in indices])\n",
    "\n",
    "        # Creating a new dataframe with categorical values split into each column\n",
    "        splitted_df = pd.DataFrame(catdf.tolist(), index=df.index, columns=indices)\n",
    "        cols_name = splitted_df.columns\n",
    "        unique_dict = dict()\n",
    "\n",
    "        # Finding the unique values in each column\n",
    "        for col in cols_name:\n",
    "            unique_dict[col] = splitted_df[col].unique()\n",
    "\n",
    "        # One hot encoding function\n",
    "        def encode(row):\n",
    "\n",
    "            encoded_col = list()\n",
    "            for key, value in unique_dict.items():\n",
    "                for cat in value:\n",
    "                    if row[key] == cat:\n",
    "                        encoded_col.append(1)\n",
    "                    else:\n",
    "                        encoded_col.append(0)\n",
    "\n",
    "            return encoded_col + noncat_df[row.name]\n",
    "\n",
    "        # Applying Encoding function\n",
    "        splitted_df['features'] = splitted_df.apply(encode, axis=1)\n",
    "        encoded_df = splitted_df.drop(columns=cols_name)\n",
    "\n",
    "        return encoded_df\n",
    "\n",
    "    def detect(self, df, k, t):\n",
    "        # K-Means Cluster Algorithm\n",
    "        cluster = KMeans(n_clusters=k, random_state=0).fit(df['features'].tolist())\n",
    "        cluster_values = cluster.labels_\n",
    "\n",
    "        # Calculating size of each cluster\n",
    "        cluster_size_dict = {}\n",
    "        for num in cluster_values:\n",
    "            if num in cluster_size_dict:\n",
    "                cluster_size_dict[num] += 1\n",
    "            else:\n",
    "                cluster_size_dict[num] = 1\n",
    "\n",
    "        # Finding cluster with max and minimum size\n",
    "        max_cluster_size = max(cluster_size_dict.values())\n",
    "        min_cluster_size = min(cluster_size_dict.values())\n",
    "\n",
    "        # confidence score logic\n",
    "        def calculate_score(row):\n",
    "            cluster_num = cluster_values[row.name]\n",
    "            score = 0\n",
    "            if (max_cluster_size - min_cluster_size) != 0:\n",
    "                score = (max_cluster_size - cluster_size_dict[cluster_num]) / (max_cluster_size - min_cluster_size)\n",
    "\n",
    "            return score\n",
    "\n",
    "        # Calculating score for each data point\n",
    "        df['score'] = df.apply(calculate_score, axis=1)\n",
    "        return df[df['score'] >= t]\n",
    "\n",
    "\n",
    "# Main Function starts here\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test using toy dataset\n",
    "    #     data = [(0, [\"http\", \"udt\", 4]), \\\n",
    "    #             (1, [\"http\", \"udf\", 5]), \\\n",
    "    #             (2, [\"http\", \"tcp\", 5]), \\\n",
    "    #             (3, [\"ftp\", \"icmp\", 1]), \\\n",
    "    #             (4, [\"http\", \"tcp\", 4])]\n",
    "\n",
    "    #     df = pd.DataFrame(data=data, columns = [\"id\", \"features\"]).set_index('id')\n",
    "\n",
    "    # Run using sample .csv example\n",
    "    df = pd.read_csv('A5-data/logs-features-sample.csv').set_index('id')\n",
    "    df['features'] = df['features'].apply(literal_eval)\n",
    "\n",
    "    ad = AnomalyDetection()\n",
    "\n",
    "    df1 = ad.cat2Num(df, [0, 1])\n",
    "    print(df1)\n",
    "    print()\n",
    "\n",
    "    df2 = ad.scaleNum(df1, range(5, 43)) # Fails for this test case\n",
    "    print(df2)\n",
    "    print()\n",
    "\n",
    "    df3 = ad.detect(df2, 8, 0.97)\n",
    "    print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda",
   "language": "python",
   "name": "tf-gpu-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
